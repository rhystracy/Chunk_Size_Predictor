{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b9657f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2429, 53)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.metrics import brier_score_loss\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "dataset = pd.read_csv('new_data/dataset_full.csv')\n",
    "\n",
    "dataset = pd.get_dummies(dataset)\n",
    "saved_cols = dataset.columns\n",
    "ace_col=0\n",
    "for col in dataset.columns:\n",
    "    if(col == 'win_reason_ace'):\n",
    "        break\n",
    "    ace_col+=1\n",
    "dataset = dataset.replace(np.nan, 0)\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "data = []\n",
    "targets = []\n",
    "\n",
    "i=0\n",
    "while(i<int(dataset.shape[0])):\n",
    "    data.append([])\n",
    "    i+=1\n",
    "i=0\n",
    "while(i<dataset.shape[0]):\n",
    "    \n",
    "    ball_round = list()\n",
    "    j=1\n",
    "    while(j<dataset.shape[1]-2):\n",
    "        ball_round.append(dataset[i,j])\n",
    "        j+=1\n",
    "    targets.append(dataset[i,j]) #team a wins label column\n",
    "    \n",
    "    data[i] = ball_round\n",
    "    \n",
    "    i+=1\n",
    "\n",
    "#print(np.asarray(data))\n",
    "data=np.asarray(data)\n",
    "targets=np.asarray(targets)\n",
    "#targets = targets.reshape(-1,1)\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c022b50",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40600f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "split_point = 2087\n",
    "\n",
    "train_x = data[0:split_point]\n",
    "train_y = targets[0:split_point]\n",
    "\n",
    "#n_classes = len(np.unique(train_y))\n",
    "\n",
    "train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n",
    "train_y = train_y.reshape(-1,1)\n",
    "\n",
    "#remove win or lose reason labels (col 38 is first win reason)\n",
    "train_x = train_x[:,:ace_col+1]\n",
    "#for i in range(train_x.shape[0]):\n",
    "#    train_x[i,56:] = 0\n",
    "\n",
    "#remove all rounds with aces\n",
    "a = train_x.shape[0]\n",
    "i=0\n",
    "while(i<a):\n",
    "    if(train_x[i,ace_col] == 0):\n",
    "        i+=1\n",
    "        continue\n",
    "    else:\n",
    "        train_x = np.delete(train_x, i, 0)\n",
    "        train_y = np.delete(train_y, i, 0)\n",
    "        a-=1\n",
    "        if(i<=split_point):\n",
    "            split_point-=1\n",
    "        continue\n",
    "train_x = train_x[:,:ace_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118a49e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encoder(inputs, head_size, num_heads, ff_dim, dropout=0): #normalize, attention, normalize, feedforward, normalize\n",
    "    # Normalization and Attention\n",
    "    #mask = np.ones((inputs.shape[1],1))\n",
    "    #mask[56:] = 0\n",
    "    #inputs = tf.boolean_mask(inputs, mask, axis=None, name='boolean_mask')\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = tf.keras.layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(dropout)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    \n",
    "    normalized_out = x + res\n",
    "    return normalized_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bdb9294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = tf.keras.layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = tf.keras.layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = tf.keras.layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7c1501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 38, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 38, 1)       2           ['input_1[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 38, 1)       7169        ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 38, 1)        0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 38, 1)       0           ['dropout[0][0]',                \n",
      " da)                                                              'input_1[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add[0][0]']   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 38, 4)        8           ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 38, 4)        0           ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)              (None, 38, 1)        5           ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 38, 1)       0           ['conv1d_1[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add_1[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 38, 1)       7169        ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 38, 1)        0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 38, 1)       0           ['dropout_2[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add_2[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)              (None, 38, 4)        8           ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 38, 4)        0           ['conv1d_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 38, 1)        5           ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TFOpLa  (None, 38, 1)       0           ['conv1d_3[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add_3[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 38, 1)       7169        ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 38, 1)        0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TFOpLa  (None, 38, 1)       0           ['dropout_4[0][0]',              \n",
      " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add_4[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 38, 4)        8           ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 38, 4)        0           ['conv1d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 38, 1)        5           ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_5 (TFOpLa  (None, 38, 1)       0           ['conv1d_5[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add_5[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 38, 1)       7169        ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 38, 1)        0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " tf.__operators__.add_6 (TFOpLa  (None, 38, 1)       0           ['dropout_6[0][0]',              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 38, 1)       2           ['tf.__operators__.add_6[0][0]'] \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 38, 4)        8           ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 38, 4)        0           ['conv1d_6[0][0]']               \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 38, 1)        5           ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " tf.__operators__.add_7 (TFOpLa  (None, 38, 1)       0           ['conv1d_7[0][0]',               \n",
      " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 38)          0           ['tf.__operators__.add_7[0][0]'] \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          4992        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            129         ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,865\n",
      "Trainable params: 33,865\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/200\n",
      "50/50 [==============================] - 11s 41ms/step - loss: 0.3370 - mean_absolute_error: 0.4786 - auc: 0.5236 - binary_accuracy: 0.5353 - val_loss: 0.2761 - val_mean_absolute_error: 0.4920 - val_auc: 0.5861 - val_binary_accuracy: 0.4987\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.3056 - mean_absolute_error: 0.4677 - auc: 0.5577 - binary_accuracy: 0.5806 - val_loss: 0.2400 - val_mean_absolute_error: 0.4560 - val_auc: 0.6079 - val_binary_accuracy: 0.6322\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.2900 - mean_absolute_error: 0.4638 - auc: 0.5667 - binary_accuracy: 0.5838 - val_loss: 0.2356 - val_mean_absolute_error: 0.4484 - val_auc: 0.6374 - val_binary_accuracy: 0.6146\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.2750 - mean_absolute_error: 0.4539 - auc: 0.5878 - binary_accuracy: 0.5938 - val_loss: 0.2272 - val_mean_absolute_error: 0.4406 - val_auc: 0.6631 - val_binary_accuracy: 0.6297\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.2589 - mean_absolute_error: 0.4469 - auc: 0.6169 - binary_accuracy: 0.6165 - val_loss: 0.2265 - val_mean_absolute_error: 0.4410 - val_auc: 0.6689 - val_binary_accuracy: 0.6322\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.2496 - mean_absolute_error: 0.4435 - auc: 0.6276 - binary_accuracy: 0.6190 - val_loss: 0.2253 - val_mean_absolute_error: 0.4379 - val_auc: 0.6715 - val_binary_accuracy: 0.6297\n",
      "Epoch 7/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.2405 - mean_absolute_error: 0.4375 - auc: 0.6502 - binary_accuracy: 0.6215 - val_loss: 0.2264 - val_mean_absolute_error: 0.4458 - val_auc: 0.6640 - val_binary_accuracy: 0.6171\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.2326 - mean_absolute_error: 0.4282 - auc: 0.6695 - binary_accuracy: 0.6429 - val_loss: 0.2227 - val_mean_absolute_error: 0.4421 - val_auc: 0.6755 - val_binary_accuracy: 0.6272\n",
      "Epoch 9/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.2295 - mean_absolute_error: 0.4342 - auc: 0.6736 - binary_accuracy: 0.6423 - val_loss: 0.2232 - val_mean_absolute_error: 0.4440 - val_auc: 0.6784 - val_binary_accuracy: 0.6196\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.2227 - mean_absolute_error: 0.4257 - auc: 0.6907 - binary_accuracy: 0.6474 - val_loss: 0.2206 - val_mean_absolute_error: 0.4433 - val_auc: 0.6807 - val_binary_accuracy: 0.6348\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.2180 - mean_absolute_error: 0.4236 - auc: 0.7021 - binary_accuracy: 0.6581 - val_loss: 0.2203 - val_mean_absolute_error: 0.4367 - val_auc: 0.6802 - val_binary_accuracy: 0.6348\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.2170 - mean_absolute_error: 0.4225 - auc: 0.7052 - binary_accuracy: 0.6637 - val_loss: 0.2193 - val_mean_absolute_error: 0.4391 - val_auc: 0.6857 - val_binary_accuracy: 0.6247\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.2146 - mean_absolute_error: 0.4198 - auc: 0.7108 - binary_accuracy: 0.6650 - val_loss: 0.2197 - val_mean_absolute_error: 0.4385 - val_auc: 0.6806 - val_binary_accuracy: 0.6474\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.2126 - mean_absolute_error: 0.4185 - auc: 0.7176 - binary_accuracy: 0.6650 - val_loss: 0.2183 - val_mean_absolute_error: 0.4386 - val_auc: 0.6899 - val_binary_accuracy: 0.6196\n",
      "Epoch 15/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.2113 - mean_absolute_error: 0.4178 - auc: 0.7215 - binary_accuracy: 0.6625 - val_loss: 0.2173 - val_mean_absolute_error: 0.4404 - val_auc: 0.6903 - val_binary_accuracy: 0.6373\n",
      "Epoch 16/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.2081 - mean_absolute_error: 0.4170 - auc: 0.7314 - binary_accuracy: 0.6820 - val_loss: 0.2187 - val_mean_absolute_error: 0.4399 - val_auc: 0.6840 - val_binary_accuracy: 0.6222\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.2074 - mean_absolute_error: 0.4169 - auc: 0.7293 - binary_accuracy: 0.6814 - val_loss: 0.2182 - val_mean_absolute_error: 0.4399 - val_auc: 0.6880 - val_binary_accuracy: 0.6373\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.2048 - mean_absolute_error: 0.4116 - auc: 0.7406 - binary_accuracy: 0.6908 - val_loss: 0.2169 - val_mean_absolute_error: 0.4361 - val_auc: 0.6869 - val_binary_accuracy: 0.6549\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.2076 - mean_absolute_error: 0.4157 - auc: 0.7332 - binary_accuracy: 0.6826 - val_loss: 0.2197 - val_mean_absolute_error: 0.4377 - val_auc: 0.6892 - val_binary_accuracy: 0.6247\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.2013 - mean_absolute_error: 0.4062 - auc: 0.7510 - binary_accuracy: 0.6927 - val_loss: 0.2183 - val_mean_absolute_error: 0.4388 - val_auc: 0.6872 - val_binary_accuracy: 0.6348\n",
      "Epoch 21/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1999 - mean_absolute_error: 0.4071 - auc: 0.7557 - binary_accuracy: 0.6946 - val_loss: 0.2160 - val_mean_absolute_error: 0.4338 - val_auc: 0.6925 - val_binary_accuracy: 0.6373\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1977 - mean_absolute_error: 0.4012 - auc: 0.7593 - binary_accuracy: 0.7059 - val_loss: 0.2174 - val_mean_absolute_error: 0.4347 - val_auc: 0.6840 - val_binary_accuracy: 0.6499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1993 - mean_absolute_error: 0.4041 - auc: 0.7564 - binary_accuracy: 0.7003 - val_loss: 0.2164 - val_mean_absolute_error: 0.4357 - val_auc: 0.6932 - val_binary_accuracy: 0.6247\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1945 - mean_absolute_error: 0.3995 - auc: 0.7692 - binary_accuracy: 0.7084 - val_loss: 0.2158 - val_mean_absolute_error: 0.4325 - val_auc: 0.6897 - val_binary_accuracy: 0.6524\n",
      "Epoch 25/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1964 - mean_absolute_error: 0.3989 - auc: 0.7639 - binary_accuracy: 0.7009 - val_loss: 0.2170 - val_mean_absolute_error: 0.4350 - val_auc: 0.6912 - val_binary_accuracy: 0.6297\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1937 - mean_absolute_error: 0.3958 - auc: 0.7697 - binary_accuracy: 0.7021 - val_loss: 0.2165 - val_mean_absolute_error: 0.4327 - val_auc: 0.6921 - val_binary_accuracy: 0.6398\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1922 - mean_absolute_error: 0.3965 - auc: 0.7781 - binary_accuracy: 0.7072 - val_loss: 0.2176 - val_mean_absolute_error: 0.4335 - val_auc: 0.6941 - val_binary_accuracy: 0.6247\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1955 - mean_absolute_error: 0.3964 - auc: 0.7648 - binary_accuracy: 0.7040 - val_loss: 0.2190 - val_mean_absolute_error: 0.4342 - val_auc: 0.6919 - val_binary_accuracy: 0.6297\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1919 - mean_absolute_error: 0.3949 - auc: 0.7768 - binary_accuracy: 0.7128 - val_loss: 0.2159 - val_mean_absolute_error: 0.4301 - val_auc: 0.6929 - val_binary_accuracy: 0.6499\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1904 - mean_absolute_error: 0.3897 - auc: 0.7796 - binary_accuracy: 0.7154 - val_loss: 0.2155 - val_mean_absolute_error: 0.4296 - val_auc: 0.6916 - val_binary_accuracy: 0.6222\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1847 - mean_absolute_error: 0.3835 - auc: 0.7941 - binary_accuracy: 0.7179 - val_loss: 0.2148 - val_mean_absolute_error: 0.4295 - val_auc: 0.6981 - val_binary_accuracy: 0.6222\n",
      "Epoch 32/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1872 - mean_absolute_error: 0.3863 - auc: 0.7890 - binary_accuracy: 0.7254 - val_loss: 0.2141 - val_mean_absolute_error: 0.4251 - val_auc: 0.6978 - val_binary_accuracy: 0.6448\n",
      "Epoch 33/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1844 - mean_absolute_error: 0.3816 - auc: 0.7949 - binary_accuracy: 0.7141 - val_loss: 0.2157 - val_mean_absolute_error: 0.4298 - val_auc: 0.7013 - val_binary_accuracy: 0.6222\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1815 - mean_absolute_error: 0.3787 - auc: 0.8028 - binary_accuracy: 0.7248 - val_loss: 0.2148 - val_mean_absolute_error: 0.4259 - val_auc: 0.7056 - val_binary_accuracy: 0.6448\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1849 - mean_absolute_error: 0.3787 - auc: 0.7926 - binary_accuracy: 0.7236 - val_loss: 0.2156 - val_mean_absolute_error: 0.4289 - val_auc: 0.7006 - val_binary_accuracy: 0.6196\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1867 - mean_absolute_error: 0.3813 - auc: 0.7887 - binary_accuracy: 0.7217 - val_loss: 0.2135 - val_mean_absolute_error: 0.4258 - val_auc: 0.7019 - val_binary_accuracy: 0.6499\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1810 - mean_absolute_error: 0.3745 - auc: 0.8019 - binary_accuracy: 0.7298 - val_loss: 0.2125 - val_mean_absolute_error: 0.4230 - val_auc: 0.7052 - val_binary_accuracy: 0.6474\n",
      "Epoch 38/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1802 - mean_absolute_error: 0.3755 - auc: 0.8061 - binary_accuracy: 0.7343 - val_loss: 0.2123 - val_mean_absolute_error: 0.4212 - val_auc: 0.7122 - val_binary_accuracy: 0.6499\n",
      "Epoch 39/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1830 - mean_absolute_error: 0.3763 - auc: 0.7984 - binary_accuracy: 0.7336 - val_loss: 0.2102 - val_mean_absolute_error: 0.4187 - val_auc: 0.7107 - val_binary_accuracy: 0.6423\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1770 - mean_absolute_error: 0.3708 - auc: 0.8139 - binary_accuracy: 0.7424 - val_loss: 0.2101 - val_mean_absolute_error: 0.4166 - val_auc: 0.7106 - val_binary_accuracy: 0.6373\n",
      "Epoch 41/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1765 - mean_absolute_error: 0.3672 - auc: 0.8137 - binary_accuracy: 0.7418 - val_loss: 0.2110 - val_mean_absolute_error: 0.4182 - val_auc: 0.7084 - val_binary_accuracy: 0.6474\n",
      "Epoch 42/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1766 - mean_absolute_error: 0.3656 - auc: 0.8118 - binary_accuracy: 0.7431 - val_loss: 0.2120 - val_mean_absolute_error: 0.4219 - val_auc: 0.7121 - val_binary_accuracy: 0.6625\n",
      "Epoch 43/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1762 - mean_absolute_error: 0.3633 - auc: 0.8119 - binary_accuracy: 0.7456 - val_loss: 0.2091 - val_mean_absolute_error: 0.4169 - val_auc: 0.7181 - val_binary_accuracy: 0.6474\n",
      "Epoch 44/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1751 - mean_absolute_error: 0.3625 - auc: 0.8155 - binary_accuracy: 0.7412 - val_loss: 0.2108 - val_mean_absolute_error: 0.4181 - val_auc: 0.7105 - val_binary_accuracy: 0.6398\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.1759 - mean_absolute_error: 0.3667 - auc: 0.8154 - binary_accuracy: 0.7393 - val_loss: 0.2088 - val_mean_absolute_error: 0.4114 - val_auc: 0.7157 - val_binary_accuracy: 0.6574\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1720 - mean_absolute_error: 0.3591 - auc: 0.8233 - binary_accuracy: 0.7626 - val_loss: 0.2122 - val_mean_absolute_error: 0.4184 - val_auc: 0.7152 - val_binary_accuracy: 0.6373\n",
      "Epoch 47/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.1743 - mean_absolute_error: 0.3604 - auc: 0.8180 - binary_accuracy: 0.7500 - val_loss: 0.2107 - val_mean_absolute_error: 0.4153 - val_auc: 0.7196 - val_binary_accuracy: 0.6398\n",
      "Epoch 48/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1707 - mean_absolute_error: 0.3572 - auc: 0.8264 - binary_accuracy: 0.7708 - val_loss: 0.2064 - val_mean_absolute_error: 0.4099 - val_auc: 0.7284 - val_binary_accuracy: 0.6675\n",
      "Epoch 49/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1712 - mean_absolute_error: 0.3589 - auc: 0.8258 - binary_accuracy: 0.7582 - val_loss: 0.2075 - val_mean_absolute_error: 0.4084 - val_auc: 0.7210 - val_binary_accuracy: 0.6499\n",
      "Epoch 50/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1683 - mean_absolute_error: 0.3498 - auc: 0.8304 - binary_accuracy: 0.7620 - val_loss: 0.2091 - val_mean_absolute_error: 0.4124 - val_auc: 0.7246 - val_binary_accuracy: 0.6675\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1699 - mean_absolute_error: 0.3530 - auc: 0.8267 - binary_accuracy: 0.7620 - val_loss: 0.2094 - val_mean_absolute_error: 0.4132 - val_auc: 0.7234 - val_binary_accuracy: 0.6423\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1688 - mean_absolute_error: 0.3530 - auc: 0.8299 - binary_accuracy: 0.7588 - val_loss: 0.2061 - val_mean_absolute_error: 0.4047 - val_auc: 0.7311 - val_binary_accuracy: 0.6549\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1686 - mean_absolute_error: 0.3488 - auc: 0.8292 - binary_accuracy: 0.7582 - val_loss: 0.2086 - val_mean_absolute_error: 0.4101 - val_auc: 0.7258 - val_binary_accuracy: 0.6423\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.1656 - mean_absolute_error: 0.3460 - auc: 0.8353 - binary_accuracy: 0.7670 - val_loss: 0.2061 - val_mean_absolute_error: 0.4088 - val_auc: 0.7329 - val_binary_accuracy: 0.6851\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1703 - mean_absolute_error: 0.3520 - auc: 0.8263 - binary_accuracy: 0.7513 - val_loss: 0.2080 - val_mean_absolute_error: 0.4083 - val_auc: 0.7352 - val_binary_accuracy: 0.6675\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1690 - mean_absolute_error: 0.3504 - auc: 0.8282 - binary_accuracy: 0.7531 - val_loss: 0.2041 - val_mean_absolute_error: 0.4059 - val_auc: 0.7367 - val_binary_accuracy: 0.6675\n",
      "Epoch 57/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1675 - mean_absolute_error: 0.3478 - auc: 0.8317 - binary_accuracy: 0.7664 - val_loss: 0.2051 - val_mean_absolute_error: 0.4022 - val_auc: 0.7311 - val_binary_accuracy: 0.6574\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1713 - mean_absolute_error: 0.3510 - auc: 0.8238 - binary_accuracy: 0.7513 - val_loss: 0.2067 - val_mean_absolute_error: 0.4073 - val_auc: 0.7361 - val_binary_accuracy: 0.6675\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1680 - mean_absolute_error: 0.3488 - auc: 0.8309 - binary_accuracy: 0.7557 - val_loss: 0.2038 - val_mean_absolute_error: 0.4024 - val_auc: 0.7406 - val_binary_accuracy: 0.6826\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - 1s 23ms/step - loss: 0.1610 - mean_absolute_error: 0.3381 - auc: 0.8456 - binary_accuracy: 0.7752 - val_loss: 0.2043 - val_mean_absolute_error: 0.4006 - val_auc: 0.7387 - val_binary_accuracy: 0.6776\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - 1s 25ms/step - loss: 0.1613 - mean_absolute_error: 0.3389 - auc: 0.8446 - binary_accuracy: 0.7796 - val_loss: 0.2069 - val_mean_absolute_error: 0.4028 - val_auc: 0.7393 - val_binary_accuracy: 0.6725\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1623 - mean_absolute_error: 0.3373 - auc: 0.8421 - binary_accuracy: 0.7720 - val_loss: 0.2044 - val_mean_absolute_error: 0.4044 - val_auc: 0.7377 - val_binary_accuracy: 0.6675\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1611 - mean_absolute_error: 0.3382 - auc: 0.8460 - binary_accuracy: 0.7783 - val_loss: 0.2037 - val_mean_absolute_error: 0.3947 - val_auc: 0.7362 - val_binary_accuracy: 0.6700\n",
      "Epoch 64/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1615 - mean_absolute_error: 0.3329 - auc: 0.8422 - binary_accuracy: 0.7733 - val_loss: 0.2010 - val_mean_absolute_error: 0.3989 - val_auc: 0.7435 - val_binary_accuracy: 0.6902\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1642 - mean_absolute_error: 0.3394 - auc: 0.8379 - binary_accuracy: 0.7727 - val_loss: 0.2038 - val_mean_absolute_error: 0.4002 - val_auc: 0.7373 - val_binary_accuracy: 0.6574\n",
      "Epoch 66/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1600 - mean_absolute_error: 0.3339 - auc: 0.8458 - binary_accuracy: 0.7739 - val_loss: 0.2005 - val_mean_absolute_error: 0.3907 - val_auc: 0.7442 - val_binary_accuracy: 0.6751\n",
      "Epoch 67/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1607 - mean_absolute_error: 0.3315 - auc: 0.8434 - binary_accuracy: 0.7720 - val_loss: 0.2012 - val_mean_absolute_error: 0.3915 - val_auc: 0.7435 - val_binary_accuracy: 0.6877\n",
      "Epoch 68/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1573 - mean_absolute_error: 0.3283 - auc: 0.8513 - binary_accuracy: 0.7764 - val_loss: 0.2007 - val_mean_absolute_error: 0.3962 - val_auc: 0.7449 - val_binary_accuracy: 0.6675\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1572 - mean_absolute_error: 0.3300 - auc: 0.8520 - binary_accuracy: 0.7821 - val_loss: 0.2041 - val_mean_absolute_error: 0.3970 - val_auc: 0.7419 - val_binary_accuracy: 0.6675\n",
      "Epoch 70/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1585 - mean_absolute_error: 0.3299 - auc: 0.8489 - binary_accuracy: 0.7827 - val_loss: 0.1998 - val_mean_absolute_error: 0.3932 - val_auc: 0.7496 - val_binary_accuracy: 0.6877\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1576 - mean_absolute_error: 0.3277 - auc: 0.8509 - binary_accuracy: 0.7783 - val_loss: 0.2006 - val_mean_absolute_error: 0.3929 - val_auc: 0.7466 - val_binary_accuracy: 0.6776\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.1563 - mean_absolute_error: 0.3264 - auc: 0.8532 - binary_accuracy: 0.7865 - val_loss: 0.2051 - val_mean_absolute_error: 0.3976 - val_auc: 0.7445 - val_binary_accuracy: 0.6776\n",
      "Epoch 73/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1551 - mean_absolute_error: 0.3219 - auc: 0.8551 - binary_accuracy: 0.7853 - val_loss: 0.1998 - val_mean_absolute_error: 0.3881 - val_auc: 0.7467 - val_binary_accuracy: 0.6826\n",
      "Epoch 74/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1548 - mean_absolute_error: 0.3230 - auc: 0.8554 - binary_accuracy: 0.7853 - val_loss: 0.2012 - val_mean_absolute_error: 0.3906 - val_auc: 0.7435 - val_binary_accuracy: 0.6776\n",
      "Epoch 75/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1564 - mean_absolute_error: 0.3279 - auc: 0.8545 - binary_accuracy: 0.7859 - val_loss: 0.2001 - val_mean_absolute_error: 0.3895 - val_auc: 0.7507 - val_binary_accuracy: 0.6977\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1541 - mean_absolute_error: 0.3183 - auc: 0.8557 - binary_accuracy: 0.7846 - val_loss: 0.1988 - val_mean_absolute_error: 0.3870 - val_auc: 0.7513 - val_binary_accuracy: 0.6952\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1561 - mean_absolute_error: 0.3250 - auc: 0.8542 - binary_accuracy: 0.7840 - val_loss: 0.2028 - val_mean_absolute_error: 0.3901 - val_auc: 0.7458 - val_binary_accuracy: 0.6751\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - 1s 19ms/step - loss: 0.1539 - mean_absolute_error: 0.3228 - auc: 0.8576 - binary_accuracy: 0.7834 - val_loss: 0.2001 - val_mean_absolute_error: 0.3821 - val_auc: 0.7460 - val_binary_accuracy: 0.6801\n",
      "Epoch 79/200\n",
      "50/50 [==============================] - 1s 20ms/step - loss: 0.1536 - mean_absolute_error: 0.3191 - auc: 0.8583 - binary_accuracy: 0.7827 - val_loss: 0.2003 - val_mean_absolute_error: 0.3868 - val_auc: 0.7533 - val_binary_accuracy: 0.6851\n",
      "Epoch 80/200\n",
      "50/50 [==============================] - 1s 21ms/step - loss: 0.1500 - mean_absolute_error: 0.3153 - auc: 0.8648 - binary_accuracy: 0.7853 - val_loss: 0.1984 - val_mean_absolute_error: 0.3871 - val_auc: 0.7522 - val_binary_accuracy: 0.6927\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.1486 - mean_absolute_error: 0.3126 - auc: 0.8675 - binary_accuracy: 0.7909 - val_loss: 0.1980 - val_mean_absolute_error: 0.3801 - val_auc: 0.7545 - val_binary_accuracy: 0.6927\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1530 - mean_absolute_error: 0.3173 - auc: 0.8588 - binary_accuracy: 0.7878 - val_loss: 0.1986 - val_mean_absolute_error: 0.3836 - val_auc: 0.7542 - val_binary_accuracy: 0.7103\n",
      "Epoch 83/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1495 - mean_absolute_error: 0.3131 - auc: 0.8635 - binary_accuracy: 0.8016 - val_loss: 0.1969 - val_mean_absolute_error: 0.3821 - val_auc: 0.7550 - val_binary_accuracy: 0.7003\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1561 - mean_absolute_error: 0.3194 - auc: 0.8528 - binary_accuracy: 0.7834 - val_loss: 0.1983 - val_mean_absolute_error: 0.3850 - val_auc: 0.7542 - val_binary_accuracy: 0.6902\n",
      "Epoch 85/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1521 - mean_absolute_error: 0.3160 - auc: 0.8592 - binary_accuracy: 0.7922 - val_loss: 0.1999 - val_mean_absolute_error: 0.3847 - val_auc: 0.7537 - val_binary_accuracy: 0.7028\n",
      "Epoch 86/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1495 - mean_absolute_error: 0.3136 - auc: 0.8662 - binary_accuracy: 0.8035 - val_loss: 0.1979 - val_mean_absolute_error: 0.3810 - val_auc: 0.7613 - val_binary_accuracy: 0.7003\n",
      "Epoch 87/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1512 - mean_absolute_error: 0.3126 - auc: 0.8611 - binary_accuracy: 0.7979 - val_loss: 0.2004 - val_mean_absolute_error: 0.3816 - val_auc: 0.7637 - val_binary_accuracy: 0.7053\n",
      "Epoch 88/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1524 - mean_absolute_error: 0.3146 - auc: 0.8597 - binary_accuracy: 0.7846 - val_loss: 0.1996 - val_mean_absolute_error: 0.3845 - val_auc: 0.7535 - val_binary_accuracy: 0.6977\n",
      "Epoch 89/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1476 - mean_absolute_error: 0.3101 - auc: 0.8690 - binary_accuracy: 0.8042 - val_loss: 0.1990 - val_mean_absolute_error: 0.3822 - val_auc: 0.7560 - val_binary_accuracy: 0.7053\n",
      "Epoch 90/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1472 - mean_absolute_error: 0.3076 - auc: 0.8681 - binary_accuracy: 0.8004 - val_loss: 0.1994 - val_mean_absolute_error: 0.3817 - val_auc: 0.7574 - val_binary_accuracy: 0.6927\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1513 - mean_absolute_error: 0.3101 - auc: 0.8601 - binary_accuracy: 0.7972 - val_loss: 0.2000 - val_mean_absolute_error: 0.3848 - val_auc: 0.7587 - val_binary_accuracy: 0.6952\n",
      "Epoch 92/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1469 - mean_absolute_error: 0.3090 - auc: 0.8708 - binary_accuracy: 0.7985 - val_loss: 0.1982 - val_mean_absolute_error: 0.3817 - val_auc: 0.7621 - val_binary_accuracy: 0.6952\n",
      "Epoch 93/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1466 - mean_absolute_error: 0.3073 - auc: 0.8700 - binary_accuracy: 0.8010 - val_loss: 0.1947 - val_mean_absolute_error: 0.3751 - val_auc: 0.7638 - val_binary_accuracy: 0.7078\n",
      "Epoch 94/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1486 - mean_absolute_error: 0.3071 - auc: 0.8652 - binary_accuracy: 0.7916 - val_loss: 0.1957 - val_mean_absolute_error: 0.3779 - val_auc: 0.7648 - val_binary_accuracy: 0.7078\n",
      "Epoch 95/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1499 - mean_absolute_error: 0.3106 - auc: 0.8640 - binary_accuracy: 0.7890 - val_loss: 0.2001 - val_mean_absolute_error: 0.3820 - val_auc: 0.7627 - val_binary_accuracy: 0.7078\n",
      "Epoch 96/200\n",
      "50/50 [==============================] - 1s 22ms/step - loss: 0.1471 - mean_absolute_error: 0.3082 - auc: 0.8696 - binary_accuracy: 0.7884 - val_loss: 0.1997 - val_mean_absolute_error: 0.3794 - val_auc: 0.7599 - val_binary_accuracy: 0.6977\n",
      "Epoch 97/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1469 - mean_absolute_error: 0.3045 - auc: 0.8685 - binary_accuracy: 0.8010 - val_loss: 0.1964 - val_mean_absolute_error: 0.3763 - val_auc: 0.7641 - val_binary_accuracy: 0.6952\n",
      "Epoch 98/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1480 - mean_absolute_error: 0.3084 - auc: 0.8679 - binary_accuracy: 0.7966 - val_loss: 0.2008 - val_mean_absolute_error: 0.3800 - val_auc: 0.7547 - val_binary_accuracy: 0.7003\n",
      "Epoch 99/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1495 - mean_absolute_error: 0.3092 - auc: 0.8644 - binary_accuracy: 0.7985 - val_loss: 0.1968 - val_mean_absolute_error: 0.3774 - val_auc: 0.7580 - val_binary_accuracy: 0.6902\n",
      "Epoch 100/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1424 - mean_absolute_error: 0.2987 - auc: 0.8774 - binary_accuracy: 0.8042 - val_loss: 0.1945 - val_mean_absolute_error: 0.3724 - val_auc: 0.7653 - val_binary_accuracy: 0.6952\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1441 - mean_absolute_error: 0.3002 - auc: 0.8738 - binary_accuracy: 0.7985 - val_loss: 0.1950 - val_mean_absolute_error: 0.3733 - val_auc: 0.7627 - val_binary_accuracy: 0.7078\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1434 - mean_absolute_error: 0.3003 - auc: 0.8755 - binary_accuracy: 0.8010 - val_loss: 0.1975 - val_mean_absolute_error: 0.3784 - val_auc: 0.7654 - val_binary_accuracy: 0.7103\n",
      "Epoch 103/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1401 - mean_absolute_error: 0.2950 - auc: 0.8815 - binary_accuracy: 0.8054 - val_loss: 0.1981 - val_mean_absolute_error: 0.3749 - val_auc: 0.7597 - val_binary_accuracy: 0.7103\n",
      "Epoch 104/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1419 - mean_absolute_error: 0.2996 - auc: 0.8782 - binary_accuracy: 0.8060 - val_loss: 0.1962 - val_mean_absolute_error: 0.3764 - val_auc: 0.7597 - val_binary_accuracy: 0.6877\n",
      "Epoch 105/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1454 - mean_absolute_error: 0.3017 - auc: 0.8728 - binary_accuracy: 0.7985 - val_loss: 0.1956 - val_mean_absolute_error: 0.3713 - val_auc: 0.7594 - val_binary_accuracy: 0.7078\n",
      "Epoch 106/200\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.1435 - mean_absolute_error: 0.2953 - auc: 0.8732 - binary_accuracy: 0.8010 - val_loss: 0.1961 - val_mean_absolute_error: 0.3758 - val_auc: 0.7607 - val_binary_accuracy: 0.7028\n",
      "Epoch 107/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1395 - mean_absolute_error: 0.2939 - auc: 0.8825 - binary_accuracy: 0.8105 - val_loss: 0.1972 - val_mean_absolute_error: 0.3743 - val_auc: 0.7623 - val_binary_accuracy: 0.7028\n",
      "Epoch 108/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1412 - mean_absolute_error: 0.2948 - auc: 0.8780 - binary_accuracy: 0.8067 - val_loss: 0.1986 - val_mean_absolute_error: 0.3739 - val_auc: 0.7521 - val_binary_accuracy: 0.6877\n",
      "Epoch 109/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1381 - mean_absolute_error: 0.2907 - auc: 0.8840 - binary_accuracy: 0.8168 - val_loss: 0.1950 - val_mean_absolute_error: 0.3703 - val_auc: 0.7629 - val_binary_accuracy: 0.7128\n",
      "Epoch 110/200\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1395 - mean_absolute_error: 0.2931 - auc: 0.8815 - binary_accuracy: 0.8155 - val_loss: 0.1958 - val_mean_absolute_error: 0.3698 - val_auc: 0.7612 - val_binary_accuracy: 0.7028\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea170dceb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = train_x.shape[1:]\n",
    "\n",
    "model = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "\n",
    "model.compile(loss=tf.losses.MeanSquaredError(),\n",
    "                optimizer=tf.optimizers.Adam(),\n",
    "                metrics=[tf.metrics.MeanAbsoluteError(), tf.metrics.AUC(), 'binary_accuracy'])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)]\n",
    "\n",
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    ")\n",
    "#model.evaluate(x_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a40a5bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "809e8cb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_x = data[split_point:]\n",
    "test_y = targets[split_point:]\n",
    "\n",
    "#n_classes = len(np.unique(train_y))\n",
    "\n",
    "test_x = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\n",
    "test_y = test_y.reshape(-1,1)\n",
    "\n",
    "#remove win or lose reason labels (col 38 is first win reason)\n",
    "test_x = test_x[:,:ace_col+1]\n",
    "#for i in range(test_x.shape[0]):\n",
    "#    test_x[i,56:] = 0\n",
    "#print(test_x.shape)\n",
    "\n",
    "#remove all rounds with aces\n",
    "a = test_x.shape[0]\n",
    "i=0\n",
    "while(i<a):\n",
    "    if(test_x[i,ace_col] == 0):\n",
    "        i+=1\n",
    "        continue\n",
    "    else:\n",
    "        test_x = np.delete(test_x, i, 0)\n",
    "        test_y = np.delete(test_y, i, 0)\n",
    "        a-=1\n",
    "        if(i<=split_point):\n",
    "            split_point-=1\n",
    "        continue\n",
    "test_x = test_x[:,:ace_col]\n",
    "\n",
    "#remove all rounds with win or lose labels\n",
    "#a = test_x.shape[0]\n",
    "#i=0\n",
    "#while(i<a):\n",
    "#    if(np.array_equal(test_x[i,55:], np.zeros((16,1)))):\n",
    "#        i+=1\n",
    "#        continue\n",
    "#    else:\n",
    "#        test_x = np.delete(test_x, i, 0)\n",
    "#        test_y = np.delete(test_y, i, 0)\n",
    "#        a-=1\n",
    "#        continue\n",
    "\n",
    "#print(test_x.shape)\n",
    "#print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8614dd22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 8ms/step - loss: 0.1809 - mean_absolute_error: 0.3557 - auc: 0.8046 - binary_accuracy: 0.7268\n",
      "\n",
      "Validation Brier Score:\n",
      "0.18089256350952884\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(test_x, test_y)\n",
    "\n",
    "val_pred = model.predict(x=test_x)\n",
    "val_pred = val_pred.reshape(val_pred.shape[0])\n",
    "val_true = test_y\n",
    "print(\"\")\n",
    "print(\"Validation Brier Score:\")\n",
    "print(brier_score_loss(y_true=val_true, y_prob=val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e2cbf4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c455ed11",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6435586 ]\n",
      " [0.28178334]\n",
      " [0.479329  ]\n",
      " [0.2600587 ]\n",
      " [0.6329782 ]\n",
      " [0.5966262 ]\n",
      " [0.32378113]\n",
      " [0.61704946]\n",
      " [0.417957  ]\n",
      " [0.47212136]\n",
      " [0.00383495]\n",
      " [0.9893833 ]\n",
      " [0.26526558]\n",
      " [0.9893833 ]\n",
      " [0.4278333 ]\n",
      " [0.3837011 ]\n",
      " [0.8143035 ]\n",
      " [0.36076063]\n",
      " [0.9893833 ]\n",
      " [0.39141598]\n",
      " [0.85522133]\n",
      " [0.6022676 ]\n",
      " [0.3958684 ]\n",
      " [0.5109517 ]\n",
      " [0.60757816]\n",
      " [0.41435602]\n",
      " [0.84785414]\n",
      " [0.9052437 ]\n",
      " [0.38363448]\n",
      " [0.22125213]\n",
      " [0.06113416]\n",
      " [0.38896298]\n",
      " [0.746829  ]\n",
      " [0.38965085]\n",
      " [0.66818655]\n",
      " [0.46344396]\n",
      " [0.75499225]\n",
      " [0.46139196]\n",
      " [0.2892806 ]\n",
      " [0.4248227 ]\n",
      " [0.505048  ]\n",
      " [0.63972807]\n",
      " [0.21049611]\n",
      " [0.9154691 ]\n",
      " [0.29957646]\n",
      " [0.4781151 ]\n",
      " [0.21066858]\n",
      " [0.28403002]\n",
      " [0.8681585 ]\n",
      " [0.523142  ]\n",
      " [0.50750303]\n",
      " [0.4615775 ]\n",
      " [0.33515516]\n",
      " [0.9893833 ]\n",
      " [0.7367187 ]\n",
      " [0.4247359 ]\n",
      " [0.867149  ]\n",
      " [0.9878974 ]\n",
      " [0.25220343]\n",
      " [0.5960123 ]\n",
      " [0.63018346]\n",
      " [0.5570171 ]\n",
      " [0.680409  ]\n",
      " [0.76454705]\n",
      " [0.2558064 ]\n",
      " [0.790987  ]\n",
      " [0.8535999 ]\n",
      " [0.4699444 ]\n",
      " [0.53238   ]\n",
      " [0.54515517]\n",
      " [0.29635277]\n",
      " [0.95443594]\n",
      " [0.20674571]\n",
      " [0.39327845]\n",
      " [0.14601423]\n",
      " [0.25251013]\n",
      " [0.19565508]\n",
      " [0.5545617 ]\n",
      " [0.81634384]\n",
      " [0.29957646]\n",
      " [0.70415455]\n",
      " [0.7333172 ]\n",
      " [0.48905683]\n",
      " [0.55773103]\n",
      " [0.29918912]\n",
      " [0.66123265]\n",
      " [0.73733264]\n",
      " [0.8541114 ]\n",
      " [0.7902835 ]\n",
      " [0.44369453]\n",
      " [0.9432323 ]\n",
      " [0.42788202]\n",
      " [0.52739024]\n",
      " [0.569812  ]\n",
      " [0.38974985]\n",
      " [0.9893833 ]\n",
      " [0.22645272]\n",
      " [0.9088946 ]\n",
      " [0.26492047]\n",
      " [0.74561363]\n",
      " [0.39674962]\n",
      " [0.817946  ]\n",
      " [0.3157839 ]\n",
      " [0.08267882]\n",
      " [0.4336359 ]\n",
      " [0.9893833 ]\n",
      " [0.31977212]\n",
      " [0.53795874]\n",
      " [0.3936402 ]\n",
      " [0.76958865]\n",
      " [0.33541846]\n",
      " [0.00383495]\n",
      " [0.9893833 ]\n",
      " [0.34772775]\n",
      " [0.9893833 ]\n",
      " [0.3482957 ]\n",
      " [0.8482206 ]\n",
      " [0.2071987 ]\n",
      " [0.5411761 ]\n",
      " [0.40518776]\n",
      " [0.6794561 ]\n",
      " [0.7147118 ]\n",
      " [0.5069978 ]\n",
      " [0.5418968 ]\n",
      " [0.90548706]\n",
      " [0.41213593]\n",
      " [0.91740346]\n",
      " [0.71248865]\n",
      " [0.28910652]\n",
      " [0.89299697]\n",
      " [0.3311892 ]\n",
      " [0.89132404]\n",
      " [0.3771098 ]\n",
      " [0.4861227 ]\n",
      " [0.67507154]\n",
      " [0.20866635]\n",
      " [0.42064357]\n",
      " [0.38495085]\n",
      " [0.8540721 ]\n",
      " [0.30826858]\n",
      " [0.49311987]\n",
      " [0.58260965]\n",
      " [0.90727544]\n",
      " [0.00383495]\n",
      " [0.88242704]\n",
      " [0.00383495]\n",
      " [0.8375285 ]\n",
      " [0.00383495]\n",
      " [0.8598931 ]\n",
      " [0.33149913]\n",
      " [0.33671606]\n",
      " [0.9893833 ]\n",
      " [0.00383495]\n",
      " [0.7250745 ]\n",
      " [0.2311845 ]\n",
      " [0.87822497]\n",
      " [0.3154748 ]\n",
      " [0.52346295]\n",
      " [0.4332635 ]\n",
      " [0.43993798]\n",
      " [0.645779  ]\n",
      " [0.25340748]\n",
      " [0.9893833 ]\n",
      " [0.19060193]\n",
      " [0.99248743]\n",
      " [0.16117232]\n",
      " [0.3871269 ]\n",
      " [0.9099458 ]\n",
      " [0.26659912]\n",
      " [0.9893833 ]\n",
      " [0.35245   ]\n",
      " [0.7205491 ]\n",
      " [0.6208252 ]\n",
      " [0.36185095]\n",
      " [0.00383495]\n",
      " [0.28505528]\n",
      " [0.3172385 ]\n",
      " [0.9893833 ]\n",
      " [0.35412   ]\n",
      " [0.4848524 ]\n",
      " [0.31025833]\n",
      " [0.03767901]\n",
      " [0.9893833 ]\n",
      " [0.00383495]\n",
      " [0.09165759]\n",
      " [0.99343807]\n",
      " [0.5425891 ]\n",
      " [0.8076934 ]\n",
      " [0.6012177 ]\n",
      " [0.9893833 ]\n",
      " [0.00383495]\n",
      " [0.4752097 ]\n",
      " [0.8120335 ]\n",
      " [0.36760297]\n",
      " [0.9893833 ]\n",
      " [0.49191576]\n",
      " [0.7317215 ]\n",
      " [0.20545805]\n",
      " [0.9893833 ]\n",
      " [0.00383495]\n",
      " [0.27787098]\n",
      " [0.9893833 ]\n",
      " [0.64960873]\n",
      " [0.7483835 ]\n",
      " [0.26957962]\n",
      " [0.8534357 ]\n",
      " [0.26768875]\n",
      " [0.03767901]\n",
      " [0.9067309 ]\n",
      " [0.9968412 ]\n",
      " [0.37651917]\n",
      " [0.40464914]\n",
      " [0.3709847 ]\n",
      " [0.6053711 ]\n",
      " [0.4251708 ]\n",
      " [0.71697885]\n",
      " [0.57125205]\n",
      " [0.55614406]\n",
      " [0.89171666]\n",
      " [0.21333939]\n",
      " [0.922584  ]\n",
      " [0.9997069 ]\n",
      " [0.5211996 ]\n",
      " [0.855662  ]\n",
      " [0.19257796]\n",
      " [0.9893833 ]\n",
      " [0.26075777]\n",
      " [0.8514366 ]\n",
      " [0.8836697 ]\n",
      " [0.39901298]\n",
      " [0.8335006 ]\n",
      " [0.99136883]\n",
      " [0.5943902 ]\n",
      " [0.5209843 ]\n",
      " [0.2546987 ]\n",
      " [0.7220372 ]\n",
      " [0.28290918]\n",
      " [0.84199876]\n",
      " [0.29656973]\n",
      " [0.4933097 ]\n",
      " [0.9893833 ]\n",
      " [0.36033848]\n",
      " [0.4002481 ]\n",
      " [0.83361363]\n",
      " [0.7436388 ]\n",
      " [0.3741318 ]\n",
      " [0.43956962]\n",
      " [0.3330608 ]\n",
      " [0.9893833 ]\n",
      " [0.00383495]\n",
      " [0.594859  ]\n",
      " [0.8264465 ]\n",
      " [0.26745024]\n",
      " [0.9893833 ]\n",
      " [0.2455936 ]\n",
      " [0.65781415]\n",
      " [0.37055773]\n",
      " [0.5386048 ]\n",
      " [0.35039306]\n",
      " [0.8678553 ]\n",
      " [0.51846486]\n",
      " [0.31840122]\n",
      " [0.19816066]\n",
      " [0.60777646]\n",
      " [0.21266374]\n",
      " [0.8237332 ]\n",
      " [0.38467613]\n",
      " [0.63258904]\n",
      " [0.49359775]\n",
      " [0.3869744 ]\n",
      " [0.7162649 ]\n",
      " [0.49186927]\n",
      " [0.96979606]\n",
      " [0.40498495]\n",
      " [0.99343807]\n",
      " [0.32953385]\n",
      " [0.85650617]\n",
      " [0.5378038 ]\n",
      " [0.9893833 ]\n",
      " [0.38242942]\n",
      " [0.71095806]\n",
      " [0.30621597]\n",
      " [0.83441436]\n",
      " [0.48121393]\n",
      " [0.7483262 ]\n",
      " [0.8335257 ]\n",
      " [0.68256044]\n",
      " [0.35059494]\n",
      " [0.732381  ]\n",
      " [0.34081307]\n",
      " [0.567703  ]\n",
      " [0.5726972 ]\n",
      " [0.00383495]\n",
      " [0.9893833 ]\n",
      " [0.00383495]\n",
      " [0.8715123 ]\n",
      " [0.8662677 ]\n",
      " [0.3108856 ]\n",
      " [0.8388878 ]\n",
      " [0.32855105]\n",
      " [0.49402454]\n",
      " [0.36738548]\n",
      " [0.9825502 ]\n",
      " [0.8063983 ]\n",
      " [0.2875293 ]\n",
      " [0.8323815 ]\n",
      " [0.39475405]\n",
      " [0.8375272 ]\n",
      " [0.38091293]\n",
      " [0.20390208]\n",
      " [0.4739725 ]\n",
      " [0.83930975]\n",
      " [0.9317214 ]\n",
      " [0.23193486]\n",
      " [0.6657658 ]\n",
      " [0.57813084]\n",
      " [0.39801028]\n",
      " [0.84248465]\n",
      " [0.46416536]\n",
      " [0.7736089 ]\n",
      " [0.3858422 ]\n",
      " [0.84688675]\n",
      " [0.69553334]\n",
      " [0.863158  ]\n",
      " [0.40476832]\n",
      " [0.5142677 ]\n",
      " [0.36220023]\n",
      " [0.29032183]\n",
      " [0.8328469 ]\n",
      " [0.26262137]\n",
      " [0.63050675]\n",
      " [0.7431897 ]\n",
      " [0.3445193 ]\n",
      " [0.99343807]\n",
      " [0.40758967]\n",
      " [0.8330684 ]\n",
      " [0.28211853]\n",
      " [0.41489428]\n",
      " [0.75838035]\n",
      " [0.38161045]\n",
      " [0.5784924 ]\n",
      " [0.60839   ]\n",
      " [0.7626582 ]\n",
      " [0.23545034]\n",
      " [0.684683  ]\n",
      " [0.5900685 ]\n",
      " [0.2534311 ]\n",
      " [0.8877375 ]\n",
      " [0.27125365]\n",
      " [0.9893833 ]\n",
      " [0.30745482]\n",
      " [0.40036234]\n",
      " [0.3251175 ]\n",
      " [0.6971808 ]\n",
      " [0.36015594]\n",
      " [0.8526673 ]\n",
      " [0.26743078]\n",
      " [0.68610084]\n",
      " [0.5329136 ]\n",
      " [0.9893833 ]\n",
      " [0.30221602]\n",
      " [0.62489253]\n",
      " [0.41321367]\n",
      " [0.62143356]\n",
      " [0.43715155]\n",
      " [0.9893833 ]\n",
      " [0.39277658]\n",
      " [0.7171869 ]\n",
      " [0.1573594 ]\n",
      " [0.70078135]\n",
      " [0.3726937 ]\n",
      " [0.813163  ]\n",
      " [0.2532356 ]\n",
      " [0.545229  ]\n",
      " [0.9163252 ]\n",
      " [0.7534812 ]\n",
      " [0.9946951 ]\n",
      " [0.84140825]\n",
      " [0.00383495]\n",
      " [0.79487836]\n",
      " [0.45369214]\n",
      " [0.75439274]\n",
      " [0.92896616]\n",
      " [0.55002654]\n",
      " [0.2858078 ]\n",
      " [0.7874498 ]\n",
      " [0.2912915 ]\n",
      " [0.8509406 ]\n",
      " [0.33020952]\n",
      " [0.6413613 ]\n",
      " [0.00383495]\n",
      " [0.6688104 ]\n",
      " [0.6319064 ]\n",
      " [0.33877987]\n",
      " [0.57901096]\n",
      " [0.4681758 ]\n",
      " [0.3142843 ]\n",
      " [0.7879994 ]\n",
      " [0.31133497]\n",
      " [0.46532157]\n",
      " [0.5012622 ]\n",
      " [0.26129642]\n",
      " [0.7643472 ]\n",
      " [0.6324902 ]\n",
      " [0.7147058 ]\n",
      " [0.67858034]\n",
      " [0.8311466 ]\n",
      " [0.49543247]\n",
      " [0.53117114]\n",
      " [0.7752638 ]]\n",
      "[[1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(pred)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02449ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
